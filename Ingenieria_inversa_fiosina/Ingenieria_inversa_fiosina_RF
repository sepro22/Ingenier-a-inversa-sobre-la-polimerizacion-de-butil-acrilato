# ===============================================
# Cargar y preparar los datos 
# ===============================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler

# Configuraci√≥n de estilo limpio para los gr√°ficos
plt.style.use('default')
plt.rcParams['font.size'] = 10
plt.rcParams['axes.linewidth'] = 1.0

# üîπ Cargar tu dataset (ajust√° el nombre del archivo o DataFrame)
df = pd.read_excel("Ingenieria_inversa_fiosina\BD_compuesta_MMD_BA_Mn_Mw.xlsx", sheet_name="mmd_raw_data")

# Variables de entrada
X_cols =[f'MMD{i+1}' for i in range(100)]
X = df[X_cols]

# Variables objetivo (condiciones de proceso)
Y_cols = ['cBA_0', 'cAIBN_0', 'temp', 'time']
Y = df[Y_cols]

# ===============================================
# Escalar datos (normalizar magnitudes)
# Los pesos moleculares tienen magnitudes altas
# El tiempo como target tambien tiene magnitudes altas
# Claramente son magnitudes altas tomando como referencia a las concentraciones de BA y AIBN
# ===============================================

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

Y_scaler = StandardScaler()
Y_scaled = Y_scaler.fit_transform(Y)

# ===============================================
# Divisi√≥n entrenamiento / prueba
# ===============================================

X_train, X_test, Y_train, Y_test = train_test_split(
    X_scaled, Y_scaled, test_size=0.2, random_state=42
)

# ===============================================
# Optimizacion de los parametros del modelo
# ===============================================

model = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

search = RandomizedSearchCV(model, param_grid, n_iter=20, cv=3, n_jobs=-1, random_state=42)
search.fit(X_train, Y_train)
best_model = search.best_estimator_

# ===============================================
# Predicci√≥n y desescalado
# ===============================================


Y_pred_scaled = best_model.predict(X_test)
Y_pred = Y_scaler.inverse_transform(Y_pred_scaled)
Y_test = Y_scaler.inverse_transform(Y_test)

# ===============================================
# M√©tricas y evaluaci√≥n
# ===============================================

mse_global = mean_squared_error(Y_test, Y_pred)
r2_global = r2_score(Y_test, Y_pred)
mae_vars = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')
r2_vars = r2_score(Y_test, Y_pred, multioutput='raw_values')

print("\nM√©tricas por variable:")
for i, col in enumerate(Y.columns):
    print(f"{col}: MAE={mae_vars[i]:.4f}, R¬≤={r2_vars[i]:.4f}")

print(f"\nMSE Global: {mse_global:.4f}")
print(f"R¬≤ Global: {r2_global:.4f}")

# ===============================================
# M√©tricas normalizadas (MAE relativo %)
# ===============================================
ranges = {
    'cBA_0': (0.5, 3.0),
    'cAIBN_0': (0.0025, 0.02),
    'temp': (60, 80),        # ajust√° seg√∫n tus datos reales
    'time': (0, 3600)
}

mae_relativo = {}
for i, col in enumerate(Y.columns):
    r_min, r_max = ranges[col]
    mae_relativo[col] = 100 * mae_vars[i] / (r_max - r_min)
    print(f"{col}: MAE relativo = {mae_relativo[col]:.2f}%")


# ===============================================
# Importancia de variables
# ===============================================
importances = best_model.feature_importances_
sorted_idx = np.argsort(importances)[-10:]

plt.figure(figsize=(8,5))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.xlabel("Importancia relativa")
plt.title("Top 10 variables m√°s influyentes")
plt.tight_layout()
plt.show()